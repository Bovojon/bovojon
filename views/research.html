<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-object-group fa-fw w3-margin-right w3-xxlarge w3-text-teal"></i>Research</h2>
  <div class="well">
    <h4>This section contains a series of blog posts about 3 areas of research that I am most interested in:</h4>
    <ul>
      <li>Natural Language Understanding (consisting of NLIDB)</li>
      <li>Word2vec</li>
      <li>BlockChain Technology</li>
    </ul>
    <p>The posts contain my annotations of the most current and relevant research papers as well as some information on experimentations that I conduct to test my ideas in those areas of reseach.</p>
  </div>
</div>

<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">October 4, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Annotation 6 (NLU)</h4>
    <p><span class="display-field">Suchanek, Fabian M., Gjergji Kasneci, and Gerhard Weikum. "Yago: A large ontology from wikipedia and wordnet." Web Semantics: Science, Services and Agents on the World Wide Web 6.3 (2008): 203-217.</span></p>
    <p> The authors of this academic article – who are from the Max-Planck-Institute for Computer Science in Saarbrücken, Germany – present YAGO, an ontology that is automatically derived from Wikipedia and WordNet. The authors begin by introducing YAGO which extends the data model of RDFS (Resource Description Framework Schema) to represent n-ary relations. They then discuss the YAGO model and the sources used to build it (Wikipedia and WordNet). The article also contains a section that
        describes how the information is extracted and it ends with the authors’ evaluation of the precision and size of YAGO, where they evaluate their use of type checking to enforce the high accuracy of the extraction heuristics and improve the precision of YAGO.
    </p>
    <p>
      Given the two main components of building an ontology are (i) extracting the information (ii) building the data model, this article will be very useful in the latter process. The information is extracted using Wikipedia’s infoboxes and category pages, in lieu of using natural language processing on the articles. However, the article does point to other academic resources that use pattern matching and natural language parsing to extract information. Hence, the ideas discussed in this article that relate to building the data models – that express entities, facts, relations between facts and properties of relations – will be most useful.
    </p>
    <hr>
  </div>
</div>

<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">October 3, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Annotation 5 (NLU)</h4>
    <p><span class="display-field">Mahesh, Kavi, and Sergei Nirenburg. Knowledge-based systems for natural language processing. New Mexico State University, Computing Research Laboratory, 1996.</span></p>
    <p> This academic article from <i>The Computer Science and Engineering Handbook</i> provides reviews of the underlying principles and methodological issues in developing knowledge-based methods for natural language processing. The authors, Kavi Mahesh from the Indian Institute of Information Technology and Sergei Nirenburg from Rensselaer Polytechnic Institute, present some examples of knowledge-based ambiguity resolution methods on selectional constraints by using grammatical knowledge or statistical knowledge. They describe the underlying principles of a Knowledge-Based Natural Language Prpcessing System (KB-NLP) and how knowledge such as that contained in selectional constraints aids NLP. They then analyze the representation and application of knowledge in KB-NLP before discussing the issues with those approaches as well as the methodological issues involved in the acquisition of both world knowledge (ontology) and lexical knowledge. A further discussion on the research issues in knowledge representation and acquisition for NLP is included towards the end. The authors present two trends (multi-engine and human assisted systems) that were used at the time of this publication in order to provide a KB-NLP with the required knowledge. The authors also devote a whole section of the article to illustrate and describe solutions to some of the well-known problems in NLP such as syntactic and word sense ambiguities.
    </p>
    <p>
      I will use this paper to (i) learn about some of the practices in KB-NLP that use semantic and world knowledge to resolve ambiguities and extract meanings of sentences (ii) discuss the issues in knowledge acquisition and representation for NLP. The paper also provides an excellent explanation of the various methods and a glossary of the terms used in the field of NLP.
    </p>
    <hr>
  </div>
</div>

<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">October 1, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Annotation 4 (NLP)</h4>
    <p><span class="display-field">Bird, Steven. "NLTK: the natural language toolkit." Proceedings of the COLING/ACL on Interactive presentation sessions. Association for Computational Linguistics, 2006.</span></p>
    <p>In this short paper, Steven Bird from the Department of Computer Science and Software Engineering at University
       of Melbourne, introduces the NLTK (Natural Language Toolkit) and explains how it is used in teaching NLP.
       The NLTK is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers,
       together with animated algorithms, tutorials, and problem sets. Bird demonstrates how to perform simple processing
       tasks with the NLTK including tokenization and stemming, tagging, and chunking and parsing.</p>
    <p>This paper serves as a very useful tutorial about the NLTK. The NLTK will be imperative in conducting research and testing hypotheses in Natural Language Processing.</p>
    <hr>
  </div>
</div>


<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">September 30, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Annotation 3 (NLU)</h4>
    <p><span class="display-field">Chaudhri, Vinay K., et al. "Three lessons for creating a knowledge base to enable explanation, reasoning and dialog." Proceedings of the Second Annual Conference on Advances in Cognitive Systems ACS. Vol. 187. 2013.</span></p>
    <p>The authors of this theoretical research paper discuss ways of systematically curating a knowledge base to enable reasoning, explanation and dialog. The three authors are researchers from the Artificial Intelligence Center, which is a laboratory in the Information and Computing Sciences Division of SRI International. The three techniques of knowledge engineering described in the paper are (1) re-formulating sentences as universal truths so that the surface form of knowledge is closer to the knowledge to be extracted (2) using a linguistically motivated ontology into which the knowledge is extracted (3) using a set of guidelines that define how various conceptual distinctions are expressed in natural language. The authors use those techniques to create a knowledge base that is later used to build an intelligent textbook that helps students learn better by answering questions, giving explanations, and engaging in dialog through natural language. In order to extract only the essential information from a body of knowledge, the paper introduces the axiom pattern known as universal truth which is a set of facts that are true for all instances of a concept. The paper explains the process of converting sentences to universal truths, then creating knowledge representation plans. A knowledge representation plan is a set of literals that appear in the consequent of the existence rule, where the rule’s antecedent has one variable that is universally quantified. The plans for a knowledge base are similar to design specification or a pseudo code for a program. Writing the plans first helps an encoder to think through the overall design of the representation before entering it into the knowledge base. The plans are then entered into the knowledge base using a graphical interface called concept maps. The authors then provide advantages of reformulating these sentences as universal truths.
    </p>
    <p>A section of the paper describes the linguistically motivated upper ontology and focuses on the taxonomy of physical actions which are subclasses of events. The authors explain how they used the linguistic grounding of the ontology to address the following two problems in the process of representing knowledge from a biology textbook: Ensuring Coverage and Choosing an Action Class. The paper ends with a discussion of the concepts introduced in the paper as well as a description of some related work. </p>
    <p>This theoretical paper can be useful in studying the ontological techniques that can be used to build a knowledge base from which an intelligent and interactive system can be created. The section that will be most useful is on the process of transforming a sentence into a knowledge representation through a series of steps. </p>
    <hr>
  </div>
</div>


<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">September 14, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Annotation 2 (Word2vec)</h4>
    <p><span class="display-field">Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." Advances in neural information processing systems. 2013.</span></p>
    <p>Researchers at Google extend on the continuous Skip-gram model by using subsampling of frequent words from text data in order to improve the quality of the word vectors and the training speed. They also present a simplified variant of Noise Contrastive Estimation (NCE) that further improves the accuracy and speed of training. Part of the research seems to relate closely to thought vectors since it discusses extensions from word-based models to phrase-based models through a data-driven approach.</p>
    <p>In introducing the main idea, the paper discusses the statistical models used in the Skip-gram model. Google’s Skip-gram model is used to learn high quality vector representations of words from large amounts of unstructured text data. Given a sequence of training words the model maximizes the average log probability. The authors use a Binary Huffman Tree data structure in the Hierarchical Softmax model to assign short codes to frequent words so that words can be grouped together by their frequency. An alternative to Hierarchical Softmax, called Negative Sampling is presented, which is a simplified version of the Noise Contrastive Estimation (NCE) that uses only samples of the noise distribution (whilst NCE uses both the samples and the numerical probabilities of the noise distribution). Another significant extension to the Skip-gram model in this research paper is the subsampling of frequent words, whereby the model learns to counter the balance of rare (e.g. “France”) and frequent (e.g. “the”, “a”) words. Additive compositionality is also explained to be used in the phrase skip-gram model.
    </p>
    <p>The empirical results of the word vector tasks show that Negative Sampling outperformed Hierarchical Softmax and the Noise Contrastive Estimation on the analogical reasoning task. Furthermore, the subsampling of the frequent words improved the training speed. The results of the Skip-gram model for phrases shows that the subsampling can result in faster training and can also improve accuracy in some cases. </p>
    <p>From a critical perspective, the authors explain the conditions in their experiments well and also compare their results to results of other neural network based models of representing words. However, the authors are biased toward the Skip-gram model and it is interesting to ask how much this academic research is influenced by the objectives of Google. </p>
    <hr>
  </div>
</div>

<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">September 8, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Annotation 1 (NLIDBS)</h4>
    <p><span class="display-field">Singh, Garima, and Arun Solanki. "An algorithm to transform natural language into SQL queries for relational databases." Selforganizology 3.3 (2016): 110-126.</span></p>
    <p>
      This paper is an engineering and empirical research paper. The authors, researchers from Gautam Buddha University, study how to process complex natural language queries and minimize ambiguity by using set of production rules and data dictionary which consists of semantics sets for relations and attributes. They explain the process of converting a natural language sentence into a SQL query through a series of steps that include lowercase conversion, tokenization, removal of escape words (words that have significance in SQL), part of speech tagging (categorizing words as nouns, pronouns, or verbs), classification of elements (putting the tokens into relations, attributes and clauses), removal of ambiguous attributes, and finally building of the query itself. The authors also discuss the results of testing their prototype and compare them with the results of an older NLIDBS. However, they do not provide any information about the older NLIDBS and the results generated for it. They also fail to clearly state what exactly their hypothesis is and what they are testing. This paper will be helpful in understanding the implementations of an NLIDBS, but there seems to be no clear evidence of the unique contribution to the field brought forward by this paper.
    </p>
    <h4 class="w3-text-teal" style="margin:30px auto;">AIML Test app</h4>
    <p>I implemented a <a href="https://github.com/Bovojon/AIML-test-app">Chatbot</a> using AIML-Python by following this amazing online <a href="http://www.devdungeon.com/content/ai-chat-bot-python-aiml" target="_blank">tutorial</a>. AIML (Artificial Intelligence Markup Language) is an XML-compliant language used for creating or customizing an Alicebot. I will be customizing and adding more functionality to the <a href="https://github.com/Bovojon/AIML-test-app">Chatbot</a> overtime.</p>
    <hr>
  </div>
</div>


<div class="w3-container w3-card-2 w3-white w3-margin-bottom animated fadeIn">
  <div class="w3-container" style="margin-top:10px">
    <h3 style="text-align:center">September 4, 2017</h3>
    <h4 class="w3-text-teal" style="margin-bottom:30px;">Introduction of ideas</h4>
    <h4 class="w3-opacity"><b>Natural Language Understanding (consisting of NLIDB)</b></h4>
    <p>
        The research for this topic constitutes the idea of communicating in natural language with an intelligent program about a specific context. There are two aspects to this idea and depending on how much work I find in those areas I will either pursue one or both of them.
        The first aspect of communicating with the intelligent program will involve making simple queries and retrieving information from the program’s knowledge base (essentially, just the READ from the CRUD operations). This can be done using a Natural Language Interface to Database System (NLIDBS) to query a knowledge base. Based on the research I have done so far, this knowledge base can be either a SQL or a NoSQL database. I am planning to use Python NLIDBS frameworks to research the methods of transforming natural language queries into SQL and/or NoSQL queries. Some of the frameworks available are python-aiml, ln2sql, and Py3kAiml. If I pursue this research idea, I will build a web application that will let users query a database of a particular knowledge base.
        The second aspect of communicating with the intelligent program will involve modifying the knowledge base in natural language (making the rest of the CRUD operations including CREATE, UPDATE, DELETE). This will involve researching Natural Language Processing, specifically Natural Language Understanding, and understanding the challenges and difficulties in modifying the database in natural language. I plan to use Python’s Natural Language Toolkit (NLTK) to study the methods used to modify an intelligent program’s knowledge base in natural language.
    </p>
    <hr>
    <h4 class="w3-opacity"><b>Word2vec</b></h4>
    <p>I am interested in researching how machine learning can be used to improve search results for a search engine. There are a number of research papers that are produced on this field by Google and they are all available online. While exploring this area of research, I discovered Google’s RankBrain, which is a Machine Learning system to embed vast amounts of natural language into mathematical vectors that the computer can understand. This led me to learning about word vectors and the tool that Google produced, called word2vec. My research will involve studying and comparing the continuous bag-of-words and skip-gram models. The word2vec tool will provide an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words.</p>
    <hr>
    <h4 class="w3-opacity"><b>BlockChain Technology</b></h4>
    <p>I am curious to study how the BlockChain technology can be applied to the sharing economy by using the idea of the ledger to provide security and to assign a digital identifier to assets. The question I am hoping to answer is: How effective is a freelance marketplace that is built on top of blockchain-based peer-to-peer network?</p>
    <hr>
  </div>
</div>
